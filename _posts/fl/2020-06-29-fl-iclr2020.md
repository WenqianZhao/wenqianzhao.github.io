---
layout: post
title: "「联邦学习最新论文解读」：ICLR2020中的联邦学习"
subtitle: "看看ICLR2020中有关联邦学习的文章"
author: "Wenqian"
header-mask: 0.4
mathjax: true
tags:
  - 机器学习
  - 深度学习
  - 联邦学习
---

## 简介
今年的ICLR2020中一共收录了7篇与联邦学习相关的文章，其中2篇是演讲的文章5篇是poster-paper，具体收录的论文标题可以看机器之心的[这篇文章](https://baijiahao.baidu.com/s?id=1665386416555022298&wfr=spider&for=pc)。这里主要介绍一下其中的三篇，其他几篇就简短地过一下。

## FAIR RESOURCE ALLOCATION IN FEDERATED LEARNING
第一篇[文章](https://arxiv.org/abs/1905.10497v2)来自CMU和FB的合作，聚焦联邦学习中的**公平性问题**。这里的**公平**问题是指在联邦学习中由于各个参与方的数据分布不同，联合训练的结果很容易导致最终的模型对于某个参与方下数据的效果比另一个参与方的要好，即所谓的“不公平”。我个人觉得公平性还是很重要的，因为这直接决定了参与方的参与热情。不过这也牵扯到一个最核心的问题，那就是——什么是公平？

文章中对于公平的定义是这样的：

> 对于训练的模型$ w $和$ \tilde{w} $，如果模型$ w $在$ m $个设备上（$ a_1,\dots,a_m $）的效果相比于$ \tilde{w} $在同样$ m $个设备上的效果更平均（uniform），那么我们就任务模型$ w $提供了一个相比于模型$ \tilde{w} $更公平的解决方案。

简单来说，就是在保证整体效果的前提下，尽量使得各个参与方之间的收益平均（方差较小）。上面的前提主要是因为作者发现公平和整体效果（这里用平均收益表示）之间有一丝矛盾。

上面提到了作者希望保证联邦学习的公平，并给出了公平的定义。那么下面就要解决两个问题（也是本文的两个最大的创新点）：如何设计一个“公平的”目标函数以及如何针对这个函数进行优化。传统的目标函数（或者说FedAvg针对的目标函数）如下图所示：

![img](/img/in-post/fl/iclr2020/fairness/obj1.png)

式子中的m表示m个设备，$ p_k $表示权重，一般设置为$ \frac{n_k}{n} $，即本地样本数量除以总样本数。$ F_k(w) $是所有本地样本上的经验风险。作者在此基础上进行了修改，提出了q-FFL：

![img](/img/in-post/fl/iclr2020/fairness/obj2.png)

其中q是一个大于0的超参数，$ F^\{q+1\}_\{k\} $表示$ F^k $的q+1次方。当q=0时，式(2)就退化为式(1)。当q越大时，整体的优化目标就越偏向那些本地经验风险较大的设备。

此外，针对FedAvg作者也提出了其对应的改进版q-FedAvg。q-FedAvg的原理相对比较复杂，这里就简单给出其优化过程：

![img](/img/in-post/fl/iclr2020/fairness/p-fedavg.png)

下面来说说这篇文章的一些局限性。我认为主要有两个，第一个就是什么才是所谓的**公平**？由于每个参与方本身提供的数据的数据量和数据质量就是不同的，我们其实不应该简单地让总体模型在各个参与方的数据上有着**尽量平均**的效果。一种显得更合理的方法是先评估各个参与方的**贡献度**，再由**贡献度**来分配收益。不过这可能又引发对小数据量参与方的歧视，而与联邦学习本身的理念背道而驰，因此如何设计一个靠谱的贡献度计算方式尤为重要。

另外超参数q的选择也是一个问题。按照作者的说法，可以先选择几组q分别进行训练，然后对比各组q对应的效果和公平性，并从中做一个取舍。但是在实际应用中，这种方式显得效率很低，并且很难达成共识。有没有更好的做法，或者有没有什么方法能够摆脱q这种超参数值得进一步研究。

不过总体来说这篇文章还是提醒了大家联邦学习中公平的重要性，并且给出了一个不错的思路，期待后续的研究进展。

## FEDERATED LEARNING WITH MATCHED AVERAGING
第二篇[文章](https://openreview.net/pdf?id=BkluqlSFDS)


## 其他几篇文章
「[GENERATIVE MODELS FOR EFFECTIVE ML ON
PRIVATE, DECENTRALIZED DATASETS](https://arxiv.org/abs/1911.06679)」来自谷歌团队，主要针对联邦学习场景下数据无法进行人工检查的问题（这里的检查是指判断标签的正确性或找出异常点等）。这篇文章提出用联邦学习场景下训练的GAN（采用差分隐私保证数据隐私）来对常见的数据问题进行检查。看上去还蛮有意思的，将来可以详细看看。

「[DBA: DISTRIBUTED BACKDOOR ATTACKS AGAINST
FEDERATED LEARNING](http://www.openreview.net/pdf?id=rkgyS0VFvr)」这篇文章的研究针对的是联邦学习的攻击模式。后门（backdoor）攻击的目的是通过注入敌对触发器（adversarial triggers）来操纵训练数据的子集，使得在被篡改数据集上训练的机器学习模型在嵌入相同触发器的测试集上会得到任意（方向）的错误预测。本文对以往**中心化的**后门攻击进行了改进，提出了一种**分布式的**后门攻击方式，即每个恶意参与方的敌对触发器不同，但是最终的攻击效果却更好。这种后门攻击在多设备的场景下是有可能发生的，不过在少量参与方参与的场景下发生的可能性较低。

「[FEDERATED ADVERSARIAL DOMAIN ADAPTATION](https://openreview.net/pdf?id=HJezF3VYPB)」这篇文章讲得是如何在联邦学习的框架下进行无监督域适应。在联邦学习中，不同参与方的数据的域（domain）很可能不同，因此联邦学习中训练得到的模型仍然可能因为域漂移的问题而无法泛化到新的设备。本文就是要解决这一问题，把对抗适应（adversarial adaptation）技术推广到联邦学习场景下。我本人对于域适应这块了解甚少，而且文章的问题设定也有点奇怪（一般来说不会是无监督），所以这篇文章就暂且先放下了。

「[DIFFERENTIALLY PRIVATE META-LEARNING](https://openreview.net/pdf?id=rJgqMRVYvr)」在元学习中加入差分隐私进行隐私保护，并且提出了一种相对宽松的隐私机制——task-global privacy，相比于 local privacy模型效果会好很多。有兴趣的朋友可以继续深入研究一下。

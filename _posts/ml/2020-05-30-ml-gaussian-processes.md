---
layout: post
title: "「机器学习中的高斯过程」：什么是高斯过程？"
subtitle: "高斯过程简介"
author: "Wenqian"
header-mask: 0.4
mathjax: true
tags:
  - 机器学习
  - 高斯过程
---

## 前言
高斯过程是概率论和数理统计中随机过程的一种，其原理相对比较复杂，却也有着比较多的应用场景。上大学时记得概率论课程中是不包含这部分的，但是由于其和机器学习的紧密关系，尤其是AutoML中很多算法会将随机过程作为代理模型，因此还是想着需要认真将这些概念学习一遍，并将我的一些感想记录于此。

> 注：本系列主要以Rasmussen & Williams的《Gaussian Processes for Machine Learning》[1]为参考和主线，对高斯过程的概念和应用做一个说明和总结。

## 从机器学习说起
我们知道机器学习主要分为无监督和有监督两种，其中有监督学习的本质就是从经验数据（也就是我们常说的训练集）中学习到一种输入输出之间的映射。通常，我们会假设某个输入数据为 $\textbf{x}$ （加粗表示一个向量），其对应的输出数据为 $y$ 。如果我们有一个存在 $n$ 个数据的数据集 $\mathcal{D}$ ，那么 $\mathcal{D}=\{\(\textbf{x}_i, y_i\)|i=1,\dots,n\}$ 。

给定上述数据集 $\mathcal{D}$ 之后，我们希望能够训练得到一个模型，使得我们可以针对数据集中未出现的某个输入 $\textbf{x}_\*$ 得到对应的预测结果。这个模型也可以看成是一个方程 $f$ ，既能够对任意输入得到一个输出，又符合某种限制条件（否则这样的 $f$ 太多了）。限制条件一般分两种：一种是限制方程的种类，比如只考虑线性方程；另一种是赋予所有可能方程一个先验概率，其值越高表示我们觉得它越可能是真的映射。第一种方法很显然存在真实方程的种类与限制种类不匹配的问题，而第二种方法则存在**如何在有限时间内计算无限可能方程**的问题。针对第二种方法的问题，我们的主角——高斯过程终于闪亮登场了。

## 简介
一个高斯过程其实就是**高斯概率分布的一种泛化**，只不过一个概率分布描述的是随机的标量或者向量，而一个随机过程针对的是方程的性质。抛开随机部分，我们可以将一个方程想象为是一个很长的向量，其中每一个元素都是一个特定输入 $x$ 所对应的输出 $f\(x\)$ 。此外高斯过程还有一个很棒的性质，那就是如果你只需要在有限个点上方程的某些性质，那么高斯过程得到的推论和你是否忽视其他无穷多个点无关。并且上述的推论和你其他任意多个请求是一致性。高斯过程的其中一大优点就在于它将一种复杂而又一致的视角和计算上的可处理性（computational tractablity）巧妙地结合了起来。

## 简单图示
事实上，机器学习中的很多模型都可以看做是高斯过程的一种特例。这里我们先看一下高斯过程是如何与贝叶斯模型相结合作用于回归或分类问题上的。

下面这张图就是上述结合的一个示例[1]。图（a）展示了从先验分布中随机抽取的四个不同的 $f\(x\)$ ，其中灰色的部分表示某一点处的两倍标准差的范围。对于先验部分，我们会给予smooth的方程更高的概率，并且所有输入的均值为0，标准差相同。这里的均值指的是对于任意的 $x$ ，其对应的 $f\(x\)$ 的均值为0。

![img](/img/in-post/ml/gp/gp1-1.png)

右侧的图（b）部分则展示了当我们有了两个观测值后的 $f\(x\)$ 的后验概率分布。预测的 $f\(x\)$ 的均值由图中的黑线表示。可以看到，观测点周围的不确定性会相对较小。因此，随着观测点的增多， $f\(x\)$ 总体的不确定性也会随之下降，不过仍然拥有一定的灵活性。

当然，这个例子还是比较简单的，它只能给你带来一个贝叶斯理论下的高斯过程的图形化展示，而缺乏很多细节。事实上同高斯分布一样，高斯过程也有着它的**均值**和**方差**（其实是方差方程），分别决定了不同 $x$ 下 $f\(x\)$ 的均值以及 $f\(x\)$ 的平滑性和稳定性。高斯过程中的**学习**通常就是找到一个合适的方差方程。

上面提到的其实是一个回归任务，对于分类任务来说，标签变成+1和-1（或者1和0），我们的目标也变为预测 $x$ 为正例的概率 $\pi\(x\)$ 。那么 $f$ 的先验也就会引出 $\pi$ 的先验。由于 $\pi$ 的取值范围为0-1，因此我们需要使用某个函数来将高斯过程回归中取值范围更大的结果映射并限定在上述范围，通常会选取sigmoid来作为映射。

高斯过程回归我们会在本系列后续的文章中进行介绍，而高斯过程分类则看情况吧~

### Reference
[1] Rasmussen C E , Williams C K I . Gaussian Processes for Machine Learning[M]. MIT Press, 2005.

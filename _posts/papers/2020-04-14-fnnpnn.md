---
layout: post
title: "「机器学习论文解读」：推荐算法-FNN和PNN"
subtitle: "如何在FM之上利用DNN做高阶特征"
author: "Wenqian"
header-mask: 0.4
tags:
  - 机器学习
  - 深度学习
  - 推荐算法
---

## 论文简介
这两篇文章的本质都是在探寻一种将DNN运用于CTR领域的方式。由于CTR场景下特征多是离散类别特征，并且每个特征的取值个数不同，在经过One-hot后往往比较稀疏，所以并不适合直接接入DNN，一来参数量太多，二来特征过于稀疏，三来无法捕获特征间的local dependency。一个很自然的想法就是对原始特征进行一种通用的处理，然后将处理后的稠密特征输入DNN进行训练，得到更高阶的隐式交叉特征。[FNN](https://arxiv.org/abs/1601.02376)和[PNN](https://arxiv.org/abs/1611.00144)其实就是给出了两种对原始特征进行处理的方案。

## FNN
FNN的想法很直观：
> CTR场景 + 通用特征工程 + 生成稠密特征 = ？ => FM！

FM似乎很适合来解决这个问题，因为它的隐向量就可以理解为是对原始特征的embedding。另外FM多用于CTR场景，也是一种通用的处理方式。由此，FNN应运而生：
![img](/img/in-post/papers/fnnpnn/fnn.png)

从上面的结构图也可以看出来，最上面几层就是典型的全连接层，而最核心的部分是最下面的**Dense Real Layer**。作者把原始稀疏特征划分为一个个field（和FFM一样），将每个field进行embedding，其初始值为【偏置+FM训练出来的隐向量】。当然这个初始值是由embedding的权重矩阵的初始值决定的。不过不得不说这篇文章的公式写的很不清楚，让人迷惑。比如下面这个最关键的公式：
![img](/img/in-post/papers/fnnpnn/fnnfml.png)

看上去对于field i，我们只得到了一个维度为K的隐向量，这和FM中对每个field中的每个特征都得到一个隐向量是不匹配的。事实上，假设field i有m个特征，这里的vi1其实可以代表这m个隐向量中的任意一个第一维的值，具体等于哪个和输入的x何处取1有关。也就是说，初始的权重矩阵Wi0其实就是[starti:endi]列隐向量，公式里的vi是依据x的取值对m个隐向量进行选择（选择取值为1的那一维度的隐向量）。

FNN给出了对原始特征进行处理的一种思路，但很显然，这其中存在两个问题：
1. FM的训练和FNN的参数训练相互割裂。
2. FM相同field下的隐向量之间没有交互，而**不同field的隐向量之间做的是加权求和而不是内积**（而且同一个隐向量的各个元素之间也会做加权求和），这对于探寻特征间的交叉可能并没有什么帮助。

前一个问题更多与工程应用相关，而第二个问题可能对模型效果影响更大。针对上面提到的两个问题，PNN进行了相应的改进。

## PNN
先看一下PNN的结构：
![img](/img/in-post/papers/fnnpnn/pnn.png)

PNN中最关键的部分就是上面的**Product Layer**。这一层包括两部分，一部分输出直接对应输入（也就是上面的z），另一部分是两两特征之间做交叉（也就是p）。这里的特征对应于前面说的field，而交叉函数则可以自己给定。作者给出两种交叉方式IP（内积）和OP（外积），分别对应两种网络IPNN和OPNN。

IP其实就是对输入的两个向量做内积，也就是说上图中p后面的每一个圆圈就是一个内积后的标量值。而OP就是对两个向量做外积，那么p后面的每个圆圈就是一个M*M的矩阵。

得到p和z之后和权重矩阵进行对位乘法得到lz和lp向量：
![img](/img/in-post/papers/fnnpnn/lplz.png)

两者加上bias后经过relu便得到了l1的结果：
![img](/img/in-post/papers/fnnpnn/l1.png)

这两种方式都有对应的手段来减少时间和空间复杂度，这里就不细说了。对于上面提到的FNN的第一个问题，PNN的做法是通过上述的结构来直接模拟FM，而不是先训练一个FM，再用其隐向量来作为embedding。事实上，文章中也有提到PNN的product层本质上是FM的一种扩展：
> if there is no hidden layer and the output layer
is simply summing up with uniform weight, PNN is identical
to FM.

另外PNN也是FNN的一种扩展，如果把lp那部分去掉，那么PNN就退化为FNN。

## PNN带来的启发
PNN给人们如何处理CTR场景下的稀疏数据带来了启发，即可以通过embedding+product层的方式对稀疏数据进行降维，并模拟FM生成二阶交叉特征。个人认为DeepFM应该就是就是顺着这个思路往下延伸的。
